{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc91896-c092-4032-9b5a-dd35ab0ecdeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Configura Path para importar 'src'\n",
    "repo_root = os.getcwd()\n",
    "repo_root = '/'.join(repo_root.split('/')[:-1])\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee42923c-6cf3-4331-9f5e-6ee7e7819af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaaf86d4-7c3e-49ac-a31c-fa4e6c8d2b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.utility.environment import Environment\n",
    "from src.infrastructure.data_manager import DataManager\n",
    "from src.infrastructure.ml_trainer import PySparkTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef64ba28-7eb3-40aa-816b-bdc3f2c37e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Defina um caminho tempor√°rio dentro do SEU volume existente\n",
    "tmp_path = Environment.MLFLOW_DFS_TMP\n",
    "\n",
    "# 2. Cria a pasta se ela n√£o existir (para evitar erros de caminho inv√°lido)\n",
    "if not os.path.exists(tmp_path):\n",
    "    os.makedirs(tmp_path)\n",
    "\n",
    "# 3. Define a vari√°vel de ambiente que o MLflow exige\n",
    "os.environ['MLFLOW_DFS_TMP'] = tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "010806c0-184d-4e21-b8d0-2f60397e9263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa m√∫ltiplos algoritmos\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "\n",
    "# 1. Carregar Dados\n",
    "manager = DataManager(spark)\n",
    "df_gold = manager.read_delta(f\"{Environment.feature_store_path}/churn\")\n",
    "\n",
    "# 2. Instancia o Trainer Gen√©rico\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "experiment_name = \"churn_initial_experiment\"\n",
    "full_experiment_path = f\"/Users/{current_user}/{experiment_name}\"\n",
    "trainer = PySparkTrainer(full_experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6385a6d6-cfc7-4a33-b55c-f3dd44ed1b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Calcule a propor√ß√£o de cada classe\n",
    "total = df_gold.count()\n",
    "count_churn = df_gold.filter(F.col('churn') == 1).count()\n",
    "count_nao_churn = total - count_churn\n",
    "\n",
    "# 2. Defina os pesos (Estrat√©gia: Inversamente proporcional √† frequ√™ncia)\n",
    "# Se Churn √© raro, ele ganha peso alto.\n",
    "peso_churn = total / (2 * count_churn)\n",
    "peso_nao_churn = total / (2 * count_nao_churn)\n",
    "\n",
    "print(f\"Peso Churn (1): {peso_churn:.2f}\")\n",
    "print(f\"Peso N√£o-Churn (0): {peso_nao_churn:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26402934-9477-48b3-9b41-088bdcc5e95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Adicione a coluna 'weight' ao DataFrame\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"weight\",\n",
    "    F.when(F.col(\"churn\") == 1, F.lit(peso_churn))\n",
    "     .otherwise(F.lit(peso_nao_churn))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48094632-ba7e-4823-bdf6-452b9873f401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Definindo as colunas utilizadas pelo modelo\n",
    "features = ['recency', 'frequency', 'monetary', 'total_items_volume', 'payment_method_count', 'max_installments', 'avg_satisfaction', 'min_review_score', 'total_reviews_given']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce8489b-bde8-4587-a463-6c8d379a8edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CEN√ÅRIO A: Treinando Random Forest (Baseline)\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"churn\", \n",
    "    featuresCol=\"features\", \n",
    "    weightCol=\"weight\",\n",
    "    numTrees=50, \n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"üöÄ Iniciando Experimento: Random Forest\")\n",
    "rf_run_id = trainer.train(df_gold, estimator=rf, run_name=\"RF_Baseline\", feature_cols=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5b464b-5dd8-4e64-8187-e15e0979416c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CEN√ÅRIO B: Treinando Gradient Boosted Trees (GBT)\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"churn\", \n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=20,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Iniciando Experimento: GBT Classifier\")\n",
    "gbt_run_id = trainer.train(df_gold, estimator=gbt, run_name=\"GBT_Baseline\", feature_cols=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc28d25-45ad-4bff-9c6c-824b7d962c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CEN√ÅRIO C: Regress√£o Log√≠stica (Mais simples e explic√°vel)\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"churn\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=1\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Iniciando Experimento: Logistic Regression\")\n",
    "lr_run_id = trainer.train(df_gold, estimator=lr, run_name=\"LogisticReg_Baseline\", feature_cols=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe7c1be8-0118-44ed-9a82-70cf156249ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Definindo um modelo para produ√ß√£o\n",
    "Como visto, o modelo deve ser avaliado frente a m√©tricas que capturam perspectivas de desbalanceamento, por isso aqui irei avaliar frente √† m√©trica F1-Score, ficando entre o GBT e o RandomForest. Ambos possuem um Throughput e uso de RAM parecidos, portanto utilizarei o GBT por ter um F1, ligeiramente maior. Por isso, ele foi adicionado como modelo de produ√ß√£o manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800f7583-2ab5-4cb5-b03f-d09cacbe3f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "003_train.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
